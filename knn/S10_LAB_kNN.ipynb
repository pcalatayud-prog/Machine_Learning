{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minería de Datos (Master en Data Science, UIMP-UC)\n",
    "\n",
    "## S10. Práctica k-NN\n",
    "\n",
    "### Rodrigo Manzanas   \n",
    "#### 2 Diciembre 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se explicó en la sesión de teoría, la técnica k-NN puede utilizarse tanto para **clasificación** como para **regresión**. En esta práctica vamos a trabajar con el dataset *MNIST* para clasificación de dígitos en imágenes y con datos meteorológicos para predecir la lluvia diaria en Lisboa. Utilizaremos los paquetes y funciones ya vistos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de la técnica k-NN para clasificación\n",
    "Para este ejemplo utilizaremos el dataset *MNIST*. Como ya habéis visto, se trata de reconocer dígitos (0, 1, ..., 9) en una colección de imágenes. En primer lugar cargamos el dataset (sólo la parte de train) con la función *read.csv* y convertimos a factor nuestra variable objetivo (primera columna). El fichero *.csv* se puede descargar de Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "data = read.csv(\".../MNIST_train.csv\")\n",
    "# converting target variable to factor\n",
    "data[,1] = as.factor(data[,1])\n",
    "str(data)\n",
    "head(data, 10)\n",
    "summary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que el dataset es muy grande (casi 4000 muestras), trabajaremos sólo con las 2000 primeras instancias. De entre éstas, nuestro dataset de train estará formado por las 1000 primeras, y el de test por las 1000 segundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train/test partition\n",
    "indtrain = 1:1000\n",
    "indtest = 1001:2000\n",
    "data.train = data[indtrain,] \n",
    "data.test = data[indtest,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer ejercicio consistirá en utilizar la función *knn* (paquete *class*) para clasificar los dígitos de test, considerando para ello los 5 vecinos más cercanos en el train. Calcula la tasa de aciertos (total) sobre el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN (with 10 nearest neighbors)\n",
    "library(class)\n",
    "pred = knn(***)\n",
    "sum(pred == ***) / length(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que el error \"global\" de clasificación de nuestro método k-NN (con k=5) está en torno al 15%. Vamos a generar ahora un barplot con la tasa de aciertos para cada dígito (0, 1, ..., 9) que nos permita hacer un ranking con los mejor y peor clasificados.  \n",
    "**Preguntas:** ¿Cuál es el dígito que mejor se clasifica? ¿Y el peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best and worst classified digits\n",
    "acc.digit = c()\n",
    "for (digit in 0:9) {\n",
    "  ind.digit = ***\n",
    "  acc.digit = c(acc.digit, ***)\n",
    "}\n",
    "bp = barplot(***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tratar de entender un poco mejor estos resultados, vamos a visualizar los 9 primeros \"1\" y los 9 primeros \"8\" que aparezcan en el dataset (*image*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nine first \"1\" images\n",
    "ind1 = which(data[,1] == 1)\n",
    "par(mfrow = c(3,3))\n",
    "for (i in 1:9) {\n",
    "  image(t(apply(matrix(as.numeric(data[ind1[i], -1]), \n",
    "                     nrow = 28, ncol = 28, byrow = TRUE),\n",
    "              2, rev)))\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nine first \"8\" images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar como influye la elección del parámetro k en nuestros resultados, repetiremos el mismo experimento pero considerando todos los k impares entre 1 y 21.   \n",
    "**Preguntas:** ¿Se obtiene alguna conclusión general sobre el efecto que tiene k en nuestras predicciones? ¿Es ese efecto más importante para algún dígito concreto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect of changing k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de la técnica k-NN para regresión \n",
    "Como ya se explicó, en el *downscaling* estadístico se trata de predecir variables meteorológicas de escala local (por ejemplo la precipitación y/o temperatura observada en un punto determinado) a partir de variables de larga escala dadas por un modelo númerico del clima cuya resolución espacial es mucho menor (por ejemplo, la presión o los vientos en dominios sinópticos). En este ejemplo utilizaremos la técnica k-NN para intentar predecir la lluvia diaria observada en Lisboa (predictando) a partir de un conjunto de variables de larga escala (predictores).  \n",
    "En primer lugar cargamos el dataset *meteo.csv*, que os podéis descargar de Moodle. Como el dataset es muy grande nos quedaremos sólo con los primeros 2000 días (filas). Además, eliminaremos la primera columna (es un simple contador). A partir de este momento tendremos un dataset (llámalo *df*) con 2000 filas y 321 columnas (compruébalo). La primera columna será nuestra variable objetivo (precipitación en Lisboa), y las 320 restantes nuestros predictores. Renombramos como *precip* nuestra variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data = read.csv(\".../meteo.csv\")\n",
    "df = data[1:2000, -1]; rm(data)\n",
    "colnames(df)[1] = \"precip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que las vamos a necesitar posteriormente, crearemos una nueva variable *y* (vector predictando con la precipitación en Lisboa) y otra *x* (matriz con los predictores de larga escala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variables x and y\n",
    "y = df[, 1]  # predictand (vector)\n",
    "x = df[, -1]  # predictors (matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar los datos, dibuja la serie diaria de precipitación observada y el promedio de los 320 predictores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "par(mfrow = c(1,2))\n",
    "plot(***)\n",
    "plot(***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Qué dirías sobre los predictores? ¿Crees que habría que tener algún cuidado especial al trabajar con la técnica k-NN en este caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro siguiente paso será dividir el dataset total en dos subconjuntos independientes de train y test (75% y 25%, respectivamente), escogidos aleatoriamente. Crea las variables *df.train*, *df.test*, *x.train*, *y.train*, *x.test*, *y.test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train/test partition\n",
    "n = nrow(df)\n",
    "indtrain = sample(1:n, round(0.75*n))\n",
    "indtest = setdiff(1:n, indtrain)\n",
    "\n",
    "# 75% train\n",
    "df.train = df[indtrain, ]\n",
    "x.train = x[indtrain, ]\n",
    "y.train = y[indtrain]\n",
    "\n",
    "# 25% test\n",
    "df.test = df[indtest, ]\n",
    "x.test = x[indtest, ]\n",
    "y.test = y[indtest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya estamos en condiciones de buscar el *k* óptimo para nuestro método k-NN. Para ello emplearemos *caret* (método *knn*). Considera una cross-validación hold-out sobre el dataset de train y barre todos los *k* impares desde 1 a 30. El argumento *preProcess* de la función *train* permite estandarizar los predictores.  \n",
    "**Pregunta:** ¿Cómo varía la métrica de validación RMSE con *k*? ¿Por qué crees que ocurre esto? ¿Cuál es el *k* óptimo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting model with caret\n",
    "library(caret)\n",
    "trctrl = trainControl(***)\n",
    "\n",
    "knn.fit = train(precip ~ ., df.train,\n",
    "                method = \"knn\",\n",
    "                trControl = trctrl,\n",
    "                preProcess = ***,\n",
    "                tuneGrid = ***)\n",
    "\n",
    "plot(***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que el RMSE disminuye con *k*. Pero para determinar cuán buena/mala es nuestra predicción no podemos fijarnos en una única medida, sino que debemos tener en cuenta un abanico más amplio de métricas que nos permitan caracterizar otros aspectos de la predicción que puedan ser de interés. En esta práctica consideraremos, además del RMSE, las siguientes métricas de validación:  \n",
    "\n",
    "* Tasa de aciertos (o *accuracy*): permite evaluar el evento binario *lluvia/no lluvia*. Se suele tomar la cantidad 0.1mm como umbral para definir días de lluvia.\n",
    "* Correlación de Spearman: permite evaluar cuán bien la serie temporal predicha (completa) sigue la observación. Se puede calcular en *R* utilizando la función *cor*.\n",
    "* *Ratio* de varianzas: permite evaluar hasta qué punto la variabilidad total de nuestra predicción (serie completa) se asemeja a la observada. Se calcula como var(pred) / var(obs).\n",
    "\n",
    "Utiliza la configuración óptima que hemos obtenido con *caret* para predecir la lluvia en el test, y valida los resultados en función de estas 4 métricas.  \n",
    "**Preguntas:** ¿Qué resultados obtienes? ¿Dirías que tu predicción es buena? ¿Mala? ¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(knn.fit, ***)\n",
    "\n",
    "## validation\n",
    "# RMSE\n",
    "rmse <- function(x, y) {\n",
    "    stopifnot(length(x) == length(y))\n",
    "    sqrt(mean((x - y)^2))\n",
    "}\n",
    "val.rmse = ***\n",
    "\n",
    "# Spearman correlation\n",
    "val.r = ***\n",
    "\n",
    "## accuracy binary\n",
    "acc.class = function(x, y) {\n",
    "  stopifnot(length(x) == length(y))\n",
    "  return(sum(diag(table(x, y))) / length(x))\n",
    "}\n",
    "val.bin = ***\n",
    "\n",
    "# ratio of variances\n",
    "val.rv = ***\n",
    "\n",
    "cbind(val.rmse, val.r, val.bin, val.rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver ahora cómo varían las 4 métricas de validación consideradas con *k*. Para ello, crea un bucle que barra todos los *k* desde 1 hasta 30 y calcula en cada iteración el RMSE, la correlación de Spearman, el accuracy (binario) y el ratio de varianzas. Plotea los resultados.  \n",
    "**Nota:** Utiliza la función *knn.reg* (paquete *FNN*). Recuerda escalar los predictores (*scale*).  \n",
    "**Preguntas:** ¿Qué conclusiones obtienes? ¿Cuál sería para tí un *k* óptimo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(FNN)\n",
    "\n",
    "## validating\n",
    "val.rmse = c()\n",
    "val.r = c()\n",
    "val.bin = c()\n",
    "val.rv = c()\n",
    "for (k in 1:30) {\n",
    "  print(sprintf(\"... trying k = %d ...\", k))\n",
    "  pred = knn.reg(train = ***, test = ***, y = ***, k = ***)\n",
    "  ***\n",
    "}\n",
    "\n",
    "## plotting\n",
    "par(mfrow = c(2,2))\n",
    "plot(***); grid()\n",
    "plot(***); grid()\n",
    "plot(***); grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sesión de teoría se comentó que es frecuente el uso de técnicas avanzadas (como el análisis de Componentes Principales) que permiten reducir la dimensionalidad de nuestro problema sin pérdida de información efectiva. En las próximas sesiones se verán en profundidad este tipo de técnicas. Sin embargo, vamos a ilustrar aquí la filosofía de las mismas con dos ejemplos muy sencillos. En primer lugar, haremos un entresacado de predictores no informado (1 de cada 5) para reducir el número de variables que entran en nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not informed regular extraction\n",
    "nx = ncol(x.train)\n",
    "ind.extr = seq(1, nx, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma un poco más dirigida de reducir la dimensionalidad de nuestro problema consiste en realizar un análisis de correlaciones. Se calcula la correlación (de Spearman) entre nuestra variable objetivo (lluvia) y todas las variables predictoras (larga escala) disponibles. La idea es que, cuanto más fuerte sea esta correlación, mayor es el vínculo físico entre predictando y predictor, y por tanto, más útil es la información que nos aporta ese predictor. Este análisis nos permite descartar aquellos predictores cuya correlación con el predictando no supere cierto umbral.  \n",
    "Siguiendo esta idea, vamos a calcular la correlación existente entre nuestro predictando y los 320 predictores disponibles, y eliminaremos aquellos con correlaciones entre -0.4 y 0.4.  \n",
    "**Pregunta:** ¿Cuánto se ha reducido la dimensionalidad de tu problema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informed selection\n",
    "r.xy = c()\n",
    "for (ivar in 2:nx) {\n",
    "  r.xy[ivar] = ***\n",
    "}\n",
    "plot(***)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como hicimos antes (utilizando todos los predictores), obtén la predicción en el test para el caso del entresacado no informado de predictores y para la selección informada de los mismos, con k desde 1 hasta 30. Evalúa estas predicciones en función de las 4 métricas de validación consideradas en el ejemplo anterior. Plotea en el mismo gráfico los resultados obtenidos para todos los predictores, el entresacado no informado de predictores y la selección informada.  \n",
    "**Pregunta:** ¿Qué conclusiones obtienes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validating\n",
    "val.extr.rmse = c()\n",
    "val.extr.r = c()\n",
    "val.extr.bin = c()\n",
    "val.extr.rv = c()\n",
    "\n",
    "val.sele.rmse = c()\n",
    "val.sele.r = c()\n",
    "val.sele.bin = c()\n",
    "val.sele.rv = c()\n",
    "\n",
    "for (k in 1:30) {\n",
    "  print(sprintf(\"... trying k = %d ...\", k))\n",
    "  \n",
    "  # not informed extraction\n",
    "  pred.extr = knn.reg(train = ***, test = ***, y = ***, k = ***)\n",
    "  ***\n",
    "  \n",
    "  # informed selection\n",
    "  pred.sele = knn.reg(train = ***, test = ***, y = ***, k = ***)\n",
    "  ***   \n",
    "}\n",
    "\n",
    "## plotting\n",
    "par(mfrow = c(2,2))\n",
    "matplot(***); grid()\n",
    "matplot(***); grid()\n",
    "matplot(***); grid()\n",
    "matplot(***); grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
